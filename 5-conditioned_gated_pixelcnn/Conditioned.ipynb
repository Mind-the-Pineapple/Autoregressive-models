{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conditioned",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDUHN7_PkeZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000a6dbd-b9e0-46b5-f169-b02ae98e49e9"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import random as rn\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vlqb0ZVlm5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Loading data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "height = 28\n",
        "width = 28\n",
        "n_channel = 1\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], height, width, n_channel)\n",
        "x_test = x_test.reshape(x_test.shape[0], height, width, n_channel)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-dvVBv0lpaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quantise(images, q_levels):\n",
        "    \"\"\"Quantise image into q levels\"\"\"\n",
        "    return (np.digitize(images, np.arange(q_levels) / q_levels) - 1).astype('float32')\n",
        "\n",
        "# Quantise the input data in q levels\n",
        "q_levels = 4\n",
        "x_train_quantised = quantise(x_train, q_levels)\n",
        "x_test_quantised = quantise(x_test, q_levels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMyl7PPyl3uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating input stream using tf.data API\n",
        "batch_size = 128\n",
        "train_buf = 60000\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_quantised / (q_levels - 1),\n",
        "                                                    x_train_quantised.astype('int32'),\n",
        "                                                    y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=train_buf)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_quantised / (q_levels - 1),\n",
        "                                                   x_test_quantised.astype('int32'),\n",
        "                                                   y_test))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NHSJmHSl88k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedConv2D(tf.keras.layers.Layer):\n",
        "    \"\"\"Convolutional layers with masks for autoregressive models\n",
        "\n",
        "    Convolutional layers with simple implementation to have masks type A and B.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 mask_type,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros'):\n",
        "        super(MaskedConv2D, self).__init__()\n",
        "\n",
        "        assert mask_type in {'A', 'B', 'V'}\n",
        "        self.mask_type = mask_type\n",
        "\n",
        "        self.filters = filters\n",
        "\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.strides = strides\n",
        "        self.padding = padding.upper()\n",
        "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "\n",
        "        self.kernel = self.add_weight(\"kernel\",\n",
        "                                      shape=(kernel_h,\n",
        "                                             kernel_w,\n",
        "                                             int(input_shape[-1]),\n",
        "                                             self.filters),\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      trainable=True)\n",
        "\n",
        "        self.bias = self.add_weight(\"bias\",\n",
        "                                    shape=(self.filters,),\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    trainable=True)\n",
        "\n",
        "        mask = np.ones(self.kernel.shape, dtype=np.float32)\n",
        "        if self.mask_type == 'V':\n",
        "            mask[kernel_h // 2:, :, :, :] = 0.\n",
        "        else:\n",
        "            mask[kernel_h // 2, kernel_w // 2 + (self.mask_type == 'B'):, :, :] = 0.\n",
        "            mask[kernel_h // 2 + 1:, :, :] = 0.\n",
        "\n",
        "        self.mask = tf.constant(mask, dtype=tf.float32, name='mask')\n",
        "\n",
        "    def call(self, input):\n",
        "        masked_kernel = tf.math.multiply(self.mask, self.kernel)\n",
        "        x = tf.nn.conv2d(input, masked_kernel, strides=[1, self.strides, self.strides, 1], padding=self.padding)\n",
        "        x = tf.nn.bias_add(x, self.bias)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp8EQOAemNyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GatedBlock(tf.keras.Model):\n",
        "    \"\"\"\"\"\"\n",
        "\n",
        "    def __init__(self, mask_type, filters, kernel_size):\n",
        "        super(GatedBlock, self).__init__(name='')\n",
        "\n",
        "        self.mask_type = mask_type\n",
        "        self.vertical_conv = MaskedConv2D(mask_type='V', filters=2 * filters, kernel_size=kernel_size)\n",
        "        self.horizontal_conv = MaskedConv2D(mask_type=mask_type, filters=2 * filters, kernel_size=(1, kernel_size))\n",
        "        self.v_to_h_conv = keras.layers.Conv2D(filters=2 * filters, kernel_size=1)\n",
        "\n",
        "        self.horizontal_output = keras.layers.Conv2D(filters=filters, kernel_size=1)\n",
        "\n",
        "        self.cond_fc_h = keras.layers.Dense(2 * filters, use_bias=False)\n",
        "        self.cond_fc_v = keras.layers.Dense(2 * filters, use_bias=False)\n",
        "\n",
        "    def _gate(self, x):\n",
        "        tanh_preactivation, sigmoid_preactivation = tf.split(x, 2, axis=-1)\n",
        "        return tf.nn.tanh(tanh_preactivation) * tf.nn.sigmoid(sigmoid_preactivation)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        v = input_tensor[0]\n",
        "        h = input_tensor[1]\n",
        "        y = input_tensor[2]\n",
        "\n",
        "        y = tf.one_hot(y, 10)\n",
        "        codified_h = tf.expand_dims(tf.expand_dims(self.cond_fc_h(y), 1), 1)\n",
        "        codified_v = tf.expand_dims(tf.expand_dims(self.cond_fc_v(y), 1), 1)\n",
        "\n",
        "        horizontal_preactivation = self.horizontal_conv(h)  # 1xN\n",
        "        vertical_preactivation = self.vertical_conv(v)  # NxN\n",
        "        v_to_h = self.v_to_h_conv(vertical_preactivation)  # 1x1\n",
        "        vertical_preactivation = vertical_preactivation+codified_v\n",
        "        v_out = self._gate(vertical_preactivation)\n",
        "\n",
        "        horizontal_preactivation = horizontal_preactivation + v_to_h\n",
        "        horizontal_preactivation = horizontal_preactivation + codified_h\n",
        "        h_activated = self._gate(horizontal_preactivation)\n",
        "\n",
        "        if self.mask_type == 'A':\n",
        "            h_out = h_activated\n",
        "        elif self.mask_type =='B':\n",
        "            h_out = self.horizontal_output(h_activated)\n",
        "            h_out = h + h_out\n",
        "\n",
        "        return v_out, h_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87_gfS-BoVRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
        "labels = keras.layers.Input(shape=(), dtype=tf.int32)\n",
        "\n",
        "v, h = GatedBlock(mask_type='A', filters=64, kernel_size=7)([inputs, inputs, labels])\n",
        "\n",
        "for i in range(7):\n",
        "    v, h = GatedBlock(mask_type='B', filters=64, kernel_size=3)([v, h, labels])\n",
        "\n",
        "x = keras.layers.Activation(activation='relu')(h)\n",
        "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
        "\n",
        "x = keras.layers.Activation(activation='relu')(x)\n",
        "x = keras.layers.Conv2D(filters=n_channel * q_levels, kernel_size=1, strides=1)(x)  # shape [N,H,W,DC]\n",
        "\n",
        "pixelcnn = tf.keras.Model(inputs=[inputs, labels], outputs=x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmml3QkbpMYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare optimizer and loss function\n",
        "lr_decay = 0.9999\n",
        "learning_rate = 5e-3\n",
        "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "compute_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF1r_8TYpQ1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(batch_x, batch_y, batch_label):\n",
        "    with tf.GradientTape() as ae_tape:\n",
        "        logits = pixelcnn([batch_x, batch_label], training=True)\n",
        "\n",
        "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])  # shape [N,H,W,DC] -> [N,H,W,D,C]\n",
        "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])  # shape [N,H,W,D,C] -> [N,H,W,C,D]\n",
        "\n",
        "        loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
        "\n",
        "    gradients = ae_tape.gradient(loss, pixelcnn.trainable_variables)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
        "    optimizer.apply_gradients(zip(gradients, pixelcnn.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKRZAARQpcjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20b4ac4e-293c-447a-9a82-b18807400f80"
      },
      "source": [
        "# Training loop\n",
        "n_epochs = 50\n",
        "n_iter = int(np.ceil(x_train_quantised.shape[0] / batch_size))\n",
        "for epoch in range(n_epochs):\n",
        "    start_epoch = time.time()\n",
        "    for i_iter, (batch_x, batch_y, batch_label) in enumerate(train_dataset):\n",
        "        start = time.time()\n",
        "        optimizer.lr = optimizer.lr * lr_decay\n",
        "        loss = train_step(batch_x, batch_y, batch_label)\n",
        "        iter_time = time.time() - start\n",
        "        if i_iter % 100 == 0:\n",
        "            print('EPOCH {:3d}: ITER {:4d}/{:4d} TIME: {:.2f} LOSS: {:.4f}'.format(epoch,\n",
        "                                                                                   i_iter, n_iter,\n",
        "                                                                                   iter_time,\n",
        "                                                                                   loss))\n",
        "    epoch_time = time.time() - start_epoch\n",
        "    print('EPOCH {:3d}: TIME: {:.2f} ETA: {:.2f}'.format(epoch,\n",
        "                                                         epoch_time,\n",
        "                                                         epoch_time * (n_epochs - epoch)))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['gated_block_7/dense_15/kernel:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['gated_block_7/dense_15/kernel:0'] when minimizing the loss.\n",
            "EPOCH   0: ITER    0/ 469 TIME: 4.71 LOSS: 1.3942\n",
            "EPOCH   0: ITER  100/ 469 TIME: 0.12 LOSS: 0.1780\n",
            "EPOCH   0: ITER  200/ 469 TIME: 0.13 LOSS: 0.1610\n",
            "EPOCH   0: ITER  300/ 469 TIME: 0.12 LOSS: 0.1598\n",
            "EPOCH   0: ITER  400/ 469 TIME: 0.12 LOSS: 0.1499\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['gated_block_7/dense_15/kernel:0'] when minimizing the loss.\n",
            "EPOCH   0: TIME: 65.28 ETA: 3264.07\n",
            "EPOCH   1: ITER    0/ 469 TIME: 0.02 LOSS: 0.1432\n",
            "EPOCH   1: ITER  100/ 469 TIME: 0.12 LOSS: 0.1451\n",
            "EPOCH   1: ITER  200/ 469 TIME: 0.12 LOSS: 0.1462\n",
            "EPOCH   1: ITER  300/ 469 TIME: 0.09 LOSS: 0.1460\n",
            "EPOCH   1: ITER  400/ 469 TIME: 0.12 LOSS: 0.1411\n",
            "EPOCH   1: TIME: 58.96 ETA: 2889.17\n",
            "EPOCH   2: ITER    0/ 469 TIME: 0.02 LOSS: 0.1419\n",
            "EPOCH   2: ITER  100/ 469 TIME: 0.12 LOSS: 0.1393\n",
            "EPOCH   2: ITER  200/ 469 TIME: 0.12 LOSS: 0.1397\n",
            "EPOCH   2: ITER  300/ 469 TIME: 0.12 LOSS: 0.1377\n",
            "EPOCH   2: ITER  400/ 469 TIME: 0.08 LOSS: 0.1401\n",
            "EPOCH   2: TIME: 58.85 ETA: 2824.79\n",
            "EPOCH   3: ITER    0/ 469 TIME: 0.02 LOSS: 0.1406\n",
            "EPOCH   3: ITER  100/ 469 TIME: 0.12 LOSS: 0.1438\n",
            "EPOCH   3: ITER  200/ 469 TIME: 0.17 LOSS: 0.1466\n",
            "EPOCH   3: ITER  300/ 469 TIME: 0.12 LOSS: 0.1385\n",
            "EPOCH   3: ITER  400/ 469 TIME: 0.12 LOSS: 0.1368\n",
            "EPOCH   3: TIME: 58.75 ETA: 2761.23\n",
            "EPOCH   4: ITER    0/ 469 TIME: 0.02 LOSS: 0.1423\n",
            "EPOCH   4: ITER  100/ 469 TIME: 0.12 LOSS: 0.1410\n",
            "EPOCH   4: ITER  200/ 469 TIME: 0.12 LOSS: 0.1434\n",
            "EPOCH   4: ITER  300/ 469 TIME: 0.17 LOSS: 0.1340\n",
            "EPOCH   4: ITER  400/ 469 TIME: 0.12 LOSS: 0.1368\n",
            "EPOCH   4: TIME: 58.78 ETA: 2703.69\n",
            "EPOCH   5: ITER    0/ 469 TIME: 0.02 LOSS: 0.1378\n",
            "EPOCH   5: ITER  100/ 469 TIME: 0.12 LOSS: 0.1412\n",
            "EPOCH   5: ITER  200/ 469 TIME: 0.12 LOSS: 0.1390\n",
            "EPOCH   5: ITER  300/ 469 TIME: 0.12 LOSS: 0.1377\n",
            "EPOCH   5: ITER  400/ 469 TIME: 0.12 LOSS: 0.1396\n",
            "EPOCH   5: TIME: 58.78 ETA: 2644.94\n",
            "EPOCH   6: ITER    0/ 469 TIME: 0.02 LOSS: 0.1382\n",
            "EPOCH   6: ITER  100/ 469 TIME: 0.12 LOSS: 0.1392\n",
            "EPOCH   6: ITER  200/ 469 TIME: 0.12 LOSS: 0.1423\n",
            "EPOCH   6: ITER  300/ 469 TIME: 0.12 LOSS: 0.1390\n",
            "EPOCH   6: ITER  400/ 469 TIME: 0.12 LOSS: 0.1382\n",
            "EPOCH   6: TIME: 58.85 ETA: 2589.52\n",
            "EPOCH   7: ITER    0/ 469 TIME: 0.02 LOSS: 0.1391\n",
            "EPOCH   7: ITER  100/ 469 TIME: 0.12 LOSS: 0.1394\n",
            "EPOCH   7: ITER  200/ 469 TIME: 0.08 LOSS: 0.1355\n",
            "EPOCH   7: ITER  300/ 469 TIME: 0.12 LOSS: 0.1363\n",
            "EPOCH   7: ITER  400/ 469 TIME: 0.13 LOSS: 0.1313\n",
            "EPOCH   7: TIME: 58.80 ETA: 2528.49\n",
            "EPOCH   8: ITER    0/ 469 TIME: 0.02 LOSS: 0.1393\n",
            "EPOCH   8: ITER  100/ 469 TIME: 0.12 LOSS: 0.1355\n",
            "EPOCH   8: ITER  200/ 469 TIME: 0.12 LOSS: 0.1310\n",
            "EPOCH   8: ITER  300/ 469 TIME: 0.12 LOSS: 0.1386\n",
            "EPOCH   8: ITER  400/ 469 TIME: 0.12 LOSS: 0.1368\n",
            "EPOCH   8: TIME: 58.85 ETA: 2471.69\n",
            "EPOCH   9: ITER    0/ 469 TIME: 0.02 LOSS: 0.1357\n",
            "EPOCH   9: ITER  100/ 469 TIME: 0.10 LOSS: 0.1410\n",
            "EPOCH   9: ITER  200/ 469 TIME: 0.11 LOSS: 0.1363\n",
            "EPOCH   9: ITER  300/ 469 TIME: 0.12 LOSS: 0.1353\n",
            "EPOCH   9: ITER  400/ 469 TIME: 0.12 LOSS: 0.1303\n",
            "EPOCH   9: TIME: 58.89 ETA: 2414.67\n",
            "EPOCH  10: ITER    0/ 469 TIME: 0.02 LOSS: 0.1374\n",
            "EPOCH  10: ITER  100/ 469 TIME: 0.12 LOSS: 0.1346\n",
            "EPOCH  10: ITER  200/ 469 TIME: 0.12 LOSS: 0.1396\n",
            "EPOCH  10: ITER  300/ 469 TIME: 0.13 LOSS: 0.1315\n",
            "EPOCH  10: ITER  400/ 469 TIME: 0.12 LOSS: 0.1337\n",
            "EPOCH  10: TIME: 58.63 ETA: 2345.08\n",
            "EPOCH  11: ITER    0/ 469 TIME: 0.02 LOSS: 0.1370\n",
            "EPOCH  11: ITER  100/ 469 TIME: 0.12 LOSS: 0.1393\n",
            "EPOCH  11: ITER  200/ 469 TIME: 0.12 LOSS: 0.1365\n",
            "EPOCH  11: ITER  300/ 469 TIME: 0.12 LOSS: 0.1354\n",
            "EPOCH  11: ITER  400/ 469 TIME: 0.12 LOSS: 0.1401\n",
            "EPOCH  11: TIME: 58.64 ETA: 2286.85\n",
            "EPOCH  12: ITER    0/ 469 TIME: 0.02 LOSS: 0.1351\n",
            "EPOCH  12: ITER  100/ 469 TIME: 0.12 LOSS: 0.1384\n",
            "EPOCH  12: ITER  200/ 469 TIME: 0.12 LOSS: 0.1304\n",
            "EPOCH  12: ITER  300/ 469 TIME: 0.12 LOSS: 0.1334\n",
            "EPOCH  12: ITER  400/ 469 TIME: 0.12 LOSS: 0.1383\n",
            "EPOCH  12: TIME: 58.65 ETA: 2228.66\n",
            "EPOCH  13: ITER    0/ 469 TIME: 0.02 LOSS: 0.1344\n",
            "EPOCH  13: ITER  100/ 469 TIME: 0.12 LOSS: 0.1370\n",
            "EPOCH  13: ITER  200/ 469 TIME: 0.12 LOSS: 0.1392\n",
            "EPOCH  13: ITER  300/ 469 TIME: 0.12 LOSS: 0.1342\n",
            "EPOCH  13: ITER  400/ 469 TIME: 0.12 LOSS: 0.1366\n",
            "EPOCH  13: TIME: 58.75 ETA: 2173.59\n",
            "EPOCH  14: ITER    0/ 469 TIME: 0.02 LOSS: 0.1388\n",
            "EPOCH  14: ITER  100/ 469 TIME: 0.12 LOSS: 0.1361\n",
            "EPOCH  14: ITER  200/ 469 TIME: 0.12 LOSS: 0.1311\n",
            "EPOCH  14: ITER  300/ 469 TIME: 0.12 LOSS: 0.1340\n",
            "EPOCH  14: ITER  400/ 469 TIME: 0.12 LOSS: 0.1352\n",
            "EPOCH  14: TIME: 58.66 ETA: 2111.87\n",
            "EPOCH  15: ITER    0/ 469 TIME: 0.02 LOSS: 0.1328\n",
            "EPOCH  15: ITER  100/ 469 TIME: 0.12 LOSS: 0.1354\n",
            "EPOCH  15: ITER  200/ 469 TIME: 0.12 LOSS: 0.1325\n",
            "EPOCH  15: ITER  300/ 469 TIME: 0.12 LOSS: 0.1346\n",
            "EPOCH  15: ITER  400/ 469 TIME: 0.12 LOSS: 0.1334\n",
            "EPOCH  15: TIME: 58.70 ETA: 2054.59\n",
            "EPOCH  16: ITER    0/ 469 TIME: 0.02 LOSS: 0.1314\n",
            "EPOCH  16: ITER  100/ 469 TIME: 0.12 LOSS: 0.1342\n",
            "EPOCH  16: ITER  200/ 469 TIME: 0.12 LOSS: 0.1351\n",
            "EPOCH  16: ITER  300/ 469 TIME: 0.12 LOSS: 0.1325\n",
            "EPOCH  16: ITER  400/ 469 TIME: 0.12 LOSS: 0.1335\n",
            "EPOCH  16: TIME: 58.68 ETA: 1995.19\n",
            "EPOCH  17: ITER    0/ 469 TIME: 0.02 LOSS: 0.1393\n",
            "EPOCH  17: ITER  100/ 469 TIME: 0.12 LOSS: 0.1375\n",
            "EPOCH  17: ITER  200/ 469 TIME: 0.12 LOSS: 0.1340\n",
            "EPOCH  17: ITER  300/ 469 TIME: 0.12 LOSS: 0.1346\n",
            "EPOCH  17: ITER  400/ 469 TIME: 0.12 LOSS: 0.1398\n",
            "EPOCH  17: TIME: 58.73 ETA: 1938.20\n",
            "EPOCH  18: ITER    0/ 469 TIME: 0.02 LOSS: 0.1315\n",
            "EPOCH  18: ITER  100/ 469 TIME: 0.12 LOSS: 0.1350\n",
            "EPOCH  18: ITER  200/ 469 TIME: 0.12 LOSS: 0.1272\n",
            "EPOCH  18: ITER  300/ 469 TIME: 0.12 LOSS: 0.1353\n",
            "EPOCH  18: ITER  400/ 469 TIME: 0.13 LOSS: 0.1335\n",
            "EPOCH  18: TIME: 58.72 ETA: 1879.08\n",
            "EPOCH  19: ITER    0/ 469 TIME: 0.02 LOSS: 0.1327\n",
            "EPOCH  19: ITER  100/ 469 TIME: 0.12 LOSS: 0.1351\n",
            "EPOCH  19: ITER  200/ 469 TIME: 0.12 LOSS: 0.1374\n",
            "EPOCH  19: ITER  300/ 469 TIME: 0.12 LOSS: 0.1350\n",
            "EPOCH  19: ITER  400/ 469 TIME: 0.12 LOSS: 0.1365\n",
            "EPOCH  19: TIME: 58.73 ETA: 1820.59\n",
            "EPOCH  20: ITER    0/ 469 TIME: 0.02 LOSS: 0.1357\n",
            "EPOCH  20: ITER  100/ 469 TIME: 0.12 LOSS: 0.1325\n",
            "EPOCH  20: ITER  200/ 469 TIME: 0.12 LOSS: 0.1322\n",
            "EPOCH  20: ITER  300/ 469 TIME: 0.12 LOSS: 0.1342\n",
            "EPOCH  20: ITER  400/ 469 TIME: 0.12 LOSS: 0.1324\n",
            "EPOCH  20: TIME: 58.73 ETA: 1761.80\n",
            "EPOCH  21: ITER    0/ 469 TIME: 0.02 LOSS: 0.1349\n",
            "EPOCH  21: ITER  100/ 469 TIME: 0.12 LOSS: 0.1294\n",
            "EPOCH  21: ITER  200/ 469 TIME: 0.12 LOSS: 0.1353\n",
            "EPOCH  21: ITER  300/ 469 TIME: 0.12 LOSS: 0.1377\n",
            "EPOCH  21: ITER  400/ 469 TIME: 0.12 LOSS: 0.1353\n",
            "EPOCH  21: TIME: 58.67 ETA: 1701.46\n",
            "EPOCH  22: ITER    0/ 469 TIME: 0.02 LOSS: 0.1281\n",
            "EPOCH  22: ITER  100/ 469 TIME: 0.12 LOSS: 0.1353\n",
            "EPOCH  22: ITER  200/ 469 TIME: 0.12 LOSS: 0.1317\n",
            "EPOCH  22: ITER  300/ 469 TIME: 0.12 LOSS: 0.1328\n",
            "EPOCH  22: ITER  400/ 469 TIME: 0.12 LOSS: 0.1288\n",
            "EPOCH  22: TIME: 58.77 ETA: 1645.57\n",
            "EPOCH  23: ITER    0/ 469 TIME: 0.02 LOSS: 0.1325\n",
            "EPOCH  23: ITER  100/ 469 TIME: 0.12 LOSS: 0.1381\n",
            "EPOCH  23: ITER  200/ 469 TIME: 0.12 LOSS: 0.1331\n",
            "EPOCH  23: ITER  300/ 469 TIME: 0.13 LOSS: 0.1357\n",
            "EPOCH  23: ITER  400/ 469 TIME: 0.12 LOSS: 0.1397\n",
            "EPOCH  23: TIME: 58.73 ETA: 1585.68\n",
            "EPOCH  24: ITER    0/ 469 TIME: 0.02 LOSS: 0.1323\n",
            "EPOCH  24: ITER  100/ 469 TIME: 0.14 LOSS: 0.1300\n",
            "EPOCH  24: ITER  200/ 469 TIME: 0.12 LOSS: 0.1283\n",
            "EPOCH  24: ITER  300/ 469 TIME: 0.12 LOSS: 0.1324\n",
            "EPOCH  24: ITER  400/ 469 TIME: 0.12 LOSS: 0.1382\n",
            "EPOCH  24: TIME: 58.71 ETA: 1526.42\n",
            "EPOCH  25: ITER    0/ 469 TIME: 0.02 LOSS: 0.1304\n",
            "EPOCH  25: ITER  100/ 469 TIME: 0.12 LOSS: 0.1355\n",
            "EPOCH  25: ITER  200/ 469 TIME: 0.12 LOSS: 0.1297\n",
            "EPOCH  25: ITER  300/ 469 TIME: 0.12 LOSS: 0.1305\n",
            "EPOCH  25: ITER  400/ 469 TIME: 0.12 LOSS: 0.1343\n",
            "EPOCH  25: TIME: 58.64 ETA: 1465.98\n",
            "EPOCH  26: ITER    0/ 469 TIME: 0.02 LOSS: 0.1360\n",
            "EPOCH  26: ITER  100/ 469 TIME: 0.12 LOSS: 0.1342\n",
            "EPOCH  26: ITER  200/ 469 TIME: 0.12 LOSS: 0.1348\n",
            "EPOCH  26: ITER  300/ 469 TIME: 0.13 LOSS: 0.1388\n",
            "EPOCH  26: ITER  400/ 469 TIME: 0.12 LOSS: 0.1346\n",
            "EPOCH  26: TIME: 58.68 ETA: 1408.25\n",
            "EPOCH  27: ITER    0/ 469 TIME: 0.02 LOSS: 0.1348\n",
            "EPOCH  27: ITER  100/ 469 TIME: 0.13 LOSS: 0.1314\n",
            "EPOCH  27: ITER  200/ 469 TIME: 0.12 LOSS: 0.1333\n",
            "EPOCH  27: ITER  300/ 469 TIME: 0.12 LOSS: 0.1335\n",
            "EPOCH  27: ITER  400/ 469 TIME: 0.12 LOSS: 0.1323\n",
            "EPOCH  27: TIME: 58.69 ETA: 1349.90\n",
            "EPOCH  28: ITER    0/ 469 TIME: 0.02 LOSS: 0.1330\n",
            "EPOCH  28: ITER  100/ 469 TIME: 0.12 LOSS: 0.1327\n",
            "EPOCH  28: ITER  200/ 469 TIME: 0.12 LOSS: 0.1326\n",
            "EPOCH  28: ITER  300/ 469 TIME: 0.12 LOSS: 0.1323\n",
            "EPOCH  28: ITER  400/ 469 TIME: 0.12 LOSS: 0.1315\n",
            "EPOCH  28: TIME: 58.69 ETA: 1291.29\n",
            "EPOCH  29: ITER    0/ 469 TIME: 0.02 LOSS: 0.1256\n",
            "EPOCH  29: ITER  100/ 469 TIME: 0.12 LOSS: 0.1317\n",
            "EPOCH  29: ITER  200/ 469 TIME: 0.12 LOSS: 0.1352\n",
            "EPOCH  29: ITER  300/ 469 TIME: 0.13 LOSS: 0.1390\n",
            "EPOCH  29: ITER  400/ 469 TIME: 0.12 LOSS: 0.1332\n",
            "EPOCH  29: TIME: 58.72 ETA: 1233.06\n",
            "EPOCH  30: ITER    0/ 469 TIME: 0.02 LOSS: 0.1286\n",
            "EPOCH  30: ITER  100/ 469 TIME: 0.12 LOSS: 0.1296\n",
            "EPOCH  30: ITER  200/ 469 TIME: 0.12 LOSS: 0.1327\n",
            "EPOCH  30: ITER  300/ 469 TIME: 0.13 LOSS: 0.1336\n",
            "EPOCH  30: ITER  400/ 469 TIME: 0.13 LOSS: 0.1298\n",
            "EPOCH  30: TIME: 58.72 ETA: 1174.35\n",
            "EPOCH  31: ITER    0/ 469 TIME: 0.02 LOSS: 0.1299\n",
            "EPOCH  31: ITER  100/ 469 TIME: 0.12 LOSS: 0.1320\n",
            "EPOCH  31: ITER  200/ 469 TIME: 0.12 LOSS: 0.1328\n",
            "EPOCH  31: ITER  300/ 469 TIME: 0.12 LOSS: 0.1270\n",
            "EPOCH  31: ITER  400/ 469 TIME: 0.12 LOSS: 0.1313\n",
            "EPOCH  31: TIME: 58.72 ETA: 1115.59\n",
            "EPOCH  32: ITER    0/ 469 TIME: 0.02 LOSS: 0.1269\n",
            "EPOCH  32: ITER  100/ 469 TIME: 0.13 LOSS: 0.1340\n",
            "EPOCH  32: ITER  200/ 469 TIME: 0.13 LOSS: 0.1353\n",
            "EPOCH  32: ITER  300/ 469 TIME: 0.12 LOSS: 0.1272\n",
            "EPOCH  32: ITER  400/ 469 TIME: 0.12 LOSS: 0.1306\n",
            "EPOCH  32: TIME: 58.74 ETA: 1057.31\n",
            "EPOCH  33: ITER    0/ 469 TIME: 0.02 LOSS: 0.1283\n",
            "EPOCH  33: ITER  100/ 469 TIME: 0.12 LOSS: 0.1325\n",
            "EPOCH  33: ITER  200/ 469 TIME: 0.12 LOSS: 0.1311\n",
            "EPOCH  33: ITER  300/ 469 TIME: 0.12 LOSS: 0.1309\n",
            "EPOCH  33: ITER  400/ 469 TIME: 0.12 LOSS: 0.1320\n",
            "EPOCH  33: TIME: 58.78 ETA: 999.27\n",
            "EPOCH  34: ITER    0/ 469 TIME: 0.02 LOSS: 0.1314\n",
            "EPOCH  34: ITER  100/ 469 TIME: 0.12 LOSS: 0.1307\n",
            "EPOCH  34: ITER  200/ 469 TIME: 0.12 LOSS: 0.1312\n",
            "EPOCH  34: ITER  300/ 469 TIME: 0.13 LOSS: 0.1263\n",
            "EPOCH  34: ITER  400/ 469 TIME: 0.12 LOSS: 0.1308\n",
            "EPOCH  34: TIME: 58.75 ETA: 940.07\n",
            "EPOCH  35: ITER    0/ 469 TIME: 0.02 LOSS: 0.1279\n",
            "EPOCH  35: ITER  100/ 469 TIME: 0.12 LOSS: 0.1255\n",
            "EPOCH  35: ITER  200/ 469 TIME: 0.12 LOSS: 0.1263\n",
            "EPOCH  35: ITER  300/ 469 TIME: 0.12 LOSS: 0.1363\n",
            "EPOCH  35: ITER  400/ 469 TIME: 0.12 LOSS: 0.1262\n",
            "EPOCH  35: TIME: 58.69 ETA: 880.39\n",
            "EPOCH  36: ITER    0/ 469 TIME: 0.02 LOSS: 0.1319\n",
            "EPOCH  36: ITER  100/ 469 TIME: 0.12 LOSS: 0.1228\n",
            "EPOCH  36: ITER  200/ 469 TIME: 0.12 LOSS: 0.1315\n",
            "EPOCH  36: ITER  300/ 469 TIME: 0.12 LOSS: 0.1380\n",
            "EPOCH  36: ITER  400/ 469 TIME: 0.12 LOSS: 0.1323\n",
            "EPOCH  36: TIME: 58.72 ETA: 822.01\n",
            "EPOCH  37: ITER    0/ 469 TIME: 0.02 LOSS: 0.1274\n",
            "EPOCH  37: ITER  100/ 469 TIME: 0.12 LOSS: 0.1281\n",
            "EPOCH  37: ITER  200/ 469 TIME: 0.12 LOSS: 0.1251\n",
            "EPOCH  37: ITER  300/ 469 TIME: 0.12 LOSS: 0.1336\n",
            "EPOCH  37: ITER  400/ 469 TIME: 0.12 LOSS: 0.1327\n",
            "EPOCH  37: TIME: 58.65 ETA: 762.49\n",
            "EPOCH  38: ITER    0/ 469 TIME: 0.02 LOSS: 0.1273\n",
            "EPOCH  38: ITER  100/ 469 TIME: 0.12 LOSS: 0.1301\n",
            "EPOCH  38: ITER  200/ 469 TIME: 0.12 LOSS: 0.1296\n",
            "EPOCH  38: ITER  300/ 469 TIME: 0.12 LOSS: 0.1282\n",
            "EPOCH  38: ITER  400/ 469 TIME: 0.12 LOSS: 0.1290\n",
            "EPOCH  38: TIME: 58.70 ETA: 704.44\n",
            "EPOCH  39: ITER    0/ 469 TIME: 0.02 LOSS: 0.1266\n",
            "EPOCH  39: ITER  100/ 469 TIME: 0.12 LOSS: 0.1313\n",
            "EPOCH  39: ITER  200/ 469 TIME: 0.12 LOSS: 0.1276\n",
            "EPOCH  39: ITER  300/ 469 TIME: 0.12 LOSS: 0.1298\n",
            "EPOCH  39: ITER  400/ 469 TIME: 0.12 LOSS: 0.1300\n",
            "EPOCH  39: TIME: 58.70 ETA: 645.73\n",
            "EPOCH  40: ITER    0/ 469 TIME: 0.02 LOSS: 0.1303\n",
            "EPOCH  40: ITER  100/ 469 TIME: 0.12 LOSS: 0.1285\n",
            "EPOCH  40: ITER  200/ 469 TIME: 0.13 LOSS: 0.1299\n",
            "EPOCH  40: ITER  300/ 469 TIME: 0.12 LOSS: 0.1267\n",
            "EPOCH  40: ITER  400/ 469 TIME: 0.12 LOSS: 0.1274\n",
            "EPOCH  40: TIME: 58.68 ETA: 586.79\n",
            "EPOCH  41: ITER    0/ 469 TIME: 0.02 LOSS: 0.1295\n",
            "EPOCH  41: ITER  100/ 469 TIME: 0.12 LOSS: 0.1321\n",
            "EPOCH  41: ITER  200/ 469 TIME: 0.12 LOSS: 0.1326\n",
            "EPOCH  41: ITER  300/ 469 TIME: 0.12 LOSS: 0.1269\n",
            "EPOCH  41: ITER  400/ 469 TIME: 0.12 LOSS: 0.1335\n",
            "EPOCH  41: TIME: 58.72 ETA: 528.44\n",
            "EPOCH  42: ITER    0/ 469 TIME: 0.02 LOSS: 0.1321\n",
            "EPOCH  42: ITER  100/ 469 TIME: 0.13 LOSS: 0.1320\n",
            "EPOCH  42: ITER  200/ 469 TIME: 0.12 LOSS: 0.1289\n",
            "EPOCH  42: ITER  300/ 469 TIME: 0.12 LOSS: 0.1288\n",
            "EPOCH  42: ITER  400/ 469 TIME: 0.12 LOSS: 0.1290\n",
            "EPOCH  42: TIME: 58.70 ETA: 469.56\n",
            "EPOCH  43: ITER    0/ 469 TIME: 0.02 LOSS: 0.1315\n",
            "EPOCH  43: ITER  100/ 469 TIME: 0.12 LOSS: 0.1273\n",
            "EPOCH  43: ITER  200/ 469 TIME: 0.12 LOSS: 0.1297\n",
            "EPOCH  43: ITER  300/ 469 TIME: 0.13 LOSS: 0.1311\n",
            "EPOCH  43: ITER  400/ 469 TIME: 0.12 LOSS: 0.1352\n",
            "EPOCH  43: TIME: 58.76 ETA: 411.32\n",
            "EPOCH  44: ITER    0/ 469 TIME: 0.02 LOSS: 0.1360\n",
            "EPOCH  44: ITER  100/ 469 TIME: 0.12 LOSS: 0.1284\n",
            "EPOCH  44: ITER  200/ 469 TIME: 0.13 LOSS: 0.1266\n",
            "EPOCH  44: ITER  300/ 469 TIME: 0.12 LOSS: 0.1286\n",
            "EPOCH  44: ITER  400/ 469 TIME: 0.12 LOSS: 0.1271\n",
            "EPOCH  44: TIME: 58.69 ETA: 352.14\n",
            "EPOCH  45: ITER    0/ 469 TIME: 0.02 LOSS: 0.1295\n",
            "EPOCH  45: ITER  100/ 469 TIME: 0.12 LOSS: 0.1303\n",
            "EPOCH  45: ITER  200/ 469 TIME: 0.12 LOSS: 0.1281\n",
            "EPOCH  45: ITER  300/ 469 TIME: 0.13 LOSS: 0.1264\n",
            "EPOCH  45: ITER  400/ 469 TIME: 0.12 LOSS: 0.1302\n",
            "EPOCH  45: TIME: 58.64 ETA: 293.21\n",
            "EPOCH  46: ITER    0/ 469 TIME: 0.02 LOSS: 0.1241\n",
            "EPOCH  46: ITER  100/ 469 TIME: 0.12 LOSS: 0.1244\n",
            "EPOCH  46: ITER  200/ 469 TIME: 0.12 LOSS: 0.1305\n",
            "EPOCH  46: ITER  300/ 469 TIME: 0.12 LOSS: 0.1278\n",
            "EPOCH  46: ITER  400/ 469 TIME: 0.13 LOSS: 0.1306\n",
            "EPOCH  46: TIME: 58.70 ETA: 234.79\n",
            "EPOCH  47: ITER    0/ 469 TIME: 0.02 LOSS: 0.1320\n",
            "EPOCH  47: ITER  100/ 469 TIME: 0.12 LOSS: 0.1317\n",
            "EPOCH  47: ITER  200/ 469 TIME: 0.12 LOSS: 0.1243\n",
            "EPOCH  47: ITER  300/ 469 TIME: 0.12 LOSS: 0.1250\n",
            "EPOCH  47: ITER  400/ 469 TIME: 0.12 LOSS: 0.1335\n",
            "EPOCH  47: TIME: 58.69 ETA: 176.07\n",
            "EPOCH  48: ITER    0/ 469 TIME: 0.02 LOSS: 0.1205\n",
            "EPOCH  48: ITER  100/ 469 TIME: 0.12 LOSS: 0.1276\n",
            "EPOCH  48: ITER  200/ 469 TIME: 0.12 LOSS: 0.1276\n",
            "EPOCH  48: ITER  300/ 469 TIME: 0.12 LOSS: 0.1293\n",
            "EPOCH  48: ITER  400/ 469 TIME: 0.12 LOSS: 0.1304\n",
            "EPOCH  48: TIME: 58.70 ETA: 117.40\n",
            "EPOCH  49: ITER    0/ 469 TIME: 0.02 LOSS: 0.1296\n",
            "EPOCH  49: ITER  100/ 469 TIME: 0.12 LOSS: 0.1258\n",
            "EPOCH  49: ITER  200/ 469 TIME: 0.12 LOSS: 0.1313\n",
            "EPOCH  49: ITER  300/ 469 TIME: 0.13 LOSS: 0.1254\n",
            "EPOCH  49: ITER  400/ 469 TIME: 0.12 LOSS: 0.1322\n",
            "EPOCH  49: TIME: 58.71 ETA: 58.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVXIo8Drkq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = np.zeros((9, height, width, n_channel), dtype='float32')\n",
        "samples_labels = (np.ones((9)) * 2).astype('int32')\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        logits = pixelcnn([samples, samples_labels])\n",
        "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])  # shape [N,H,W,DC] -> [N,H,W,D,C]\n",
        "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])  # shape [N,H,W,D,C] -> [N,H,W,C,D]\n",
        "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
        "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVVSOeButHG6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "440e4a53-7899-49f4-fa1d-f1589c5d388b"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3, 3, i + 1)\n",
        "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
        "    plt.xticks(np.array([]))\n",
        "    plt.yticks(np.array([]))\n",
        "plt.show()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAAC4CAYAAABQMybHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAJ00lEQVR4nO2dX7LbKgzG4c5dQvpc7yHdCl4k3kqy\niD63e+A+9NLhEKyIhD9C+X4znnbO8UmQLX9IgIUNIRgAtPLP7AYA0BM4OFANHByoBg4OVAMHB6qB\ngwPVwMGBauDgQDVwcKCaf2tOvlwuYdu2Tk3pz/1+/x1C+MY5F7auA2VrlYNv22Zut1ubVk3AWvuT\ney5sXQfKVoQoQDVwcKAaODhQDRwcqAYODlQDBweqgYMD1Qxx8OM4jLX2y3EchzmOY8TXg8WIvrHv\n+4Pf1FI10fMq+76TP3POFf8FuqBErZfYIUQBqhmi4M+IT2/pKV7trf/jOJr1QCXF8943+ewRxLaX\nevBaXr2mUHCgmmkKHp/IVKHSpzT+PCYWUbmkxecllXq3rVQylf4uV3NJ12bf96q42jn3kIPVfkaJ\n4Q4eG/+sq80dJ/4rLWQp3YDYVq6jx/Nrb2ap6x95fUrhWO1IR6m9pZD11dAMIQpQjYgks8SZ6u37\nLirRim0pKRdHyVt0w5FRIUpqaynUpKB6cCrcexUoOFDNEAVvGRdKnf2MNpYUOVfy9GfvMHqCLNqV\nqjbnfnDyrjhzefa3rwIFB6oRG4NHojK+sg5hBt7701GRFqpdYlTsnfcU1D1xzrHiZ+ozWvT84h18\nReKN5Q7/cbvw/LNmJ9shhAfbWgyJtgxpEaIA3YQQ2Mf1eg2zMMb8Pd74jFuYaKv3PjjngnMueO+D\n9775d0Rm21rCOfflPqbHO9eCshUKDlSDGHwg6XqLT4CzmrD3EKd4B195uegnczY6MnrRHEIUoJrp\nCn62XLb0+7NzgByocMR7P/z+QcGBaoYrODfxqF2lBubAuZ9h4hp+KDhQzXAFz5/00gq70iq1mSoA\nyjxbyy4hX5r+TmZ6EaITr7Kw6lOhqiBEpAgSQhSgmunDhClUolJ6aQDMgQo9pN0fKDhQjQgFp9Y6\nv1pSAYyBWwZkFtPqoqROXRpZyZPQtBBQnsCUnF9CBq8dKYkkBUIUoJrhCp7XETmOgyxBVqo7whlG\nHK0uaS/EWQFZ6nWkdvMrAwUHqpmWZNYq7LNJoFnFOfME+ax6FYdo2wqx7SpAwYFqRAwT1hBC+KuI\nqVrOGjWJ3/uO6ua9Eia12rGcgxuj78bn4Vca9mC48z0QogDV2Jqu1Vr7yxjzs19zuvM9hPCNcyJs\nXYpTW6scHIDVQIgCVAMHB6qBgwPVwMGBauDgQDVwcKAaODhQDRwcqKZqLcrlcgnbtnVqSn/u9/tv\n7uwebF0HytYqB9+2zdxutzatmoC1lj0dDVvXgbIVIQpQDRwcqAYODlQj9oWHs/cYnXPYtwewgYID\n1YhUcKruNEq4gRqg4EA1IhScs88LkMcKNSFFODjHsdMLl19Y6W+fUxVyZ2yt9y6cir9S7EKIAnQT\nQmAf1+s1tMY5F4wxXw7nXPDeB+/9w/ne+4fzuRhjbmGArc65ol1nx5mt79DT1vQ+RaINo+zL2nNq\nKxQcqGZ6DF7ayrs0cVOK+yTEeC12oKAKd0qkVOg0L2GXDvWmpehG2zjNwbn1sakt62Y7xLM65VTF\n25VHjjjX3Xv/8PDv+z68ci5CFKCa6RvBPuu2SgpX2kR2JM+Um9O+2b3PCEobiY2ugQ4FB6qZnmSe\nKdlZ0uacm7Z6kJNIfoIy15IqeSnx7AkUHKhmuoKfkcfe0jccjWC14zne+4ci/70R6eBUYjkTbhs4\n5+FBGANCFKAaUQpemhWctT0gRTrE9epMZtpLSbJNG1BwoBpRCl5aZyJd3WIPkw5/UWouNb8YwYy8\nQ4SDl2YGpY+W5KSLjagbmf9uZefmrqPhPPC97jdCFKCargpOrQR89rrTypypcknxVrK1xwrI+Jlp\nL56Gp7kP1Q46QMGBapor+HEcpHJz2Pdd5PDgq0h9WaMWSrmpNe/U+VTvXvpdbAN3NSIUHKimuYI/\nizO56r7qWhRjHm3kvr20ImdKmvbktee3zM+aOXjJsUvGcF89K9U+MeZPMiLR2bkJWD5unrJi2JLb\nQY3zU/fLOdfFfoQoQDXNFJxKoI7jIMshU092qbuXpNy1a1E4Q2xSE+xSYkjZI8EOKDhQzdsKTj3B\nz2ryGfP86V5lTUpLaofCRpHnD6VEMr1fEu5Zl5lMThYswXjpWGvFObkxa4kOQhSgmrcVvFT7gjpv\nhae+Bs5sXe2rbquVcpMMFByoplkMniYg+covzcQYueWadih3O6DgQDXNR1GkDA+NRtrwGPiDiFfW\nNPAJ4diKIEQBqrE1EwnW2l/GmJ/9mtOd7yGEb5wTYetSnNpa5eAArAZCFKAaODhQDRwcqAYODlQD\nBweqgYMD1cDBgWqqpuovl0vYtq1TU/pzv99/cyc/YOs6ULZWOfi2beZ2u7Vp1QSstezZOti6DpSt\nCFGAauDgQDVwcKAaOHhH4ut71lpjrW1aOP5TiNcuHmdFPc+AgwPVDHmjp/RCbuRZbcIVOas0G9Xc\nGHlVq2rgVNL13nd5dQ8F8AFImP5OZlp5VpuSU6xY3KdmE6pe29BUf1YIgX1cr9fwCsYY1tEbY8wt\ndLb1/+/5cjjngnNuqM2tbfXes+9jfnjvg/e+2obSNau1FSEKUM2QECUNPfIuJt36Wnuoku77qIHS\nAEEpjKlNDEt79bwa5kDBgWqGKDj36dOkbtpwzrEUON7rEMLD8PBxHCxfoDayqmX6KMonUCpMGuHe\n9BXJS2vv+05uI0htQYkQBYACUPAOcJMp7dTsf3q2j9O7vRsUHKhmmoJzt/QGuuDMgrbsAaHgQDXT\nFJzKmLVBreHQOoLCpffmZMMdvLR0dqV9F2uAY/8h3T0u/n+UmCFEAaoZpuBnLz180gsPKdpspphp\nKxQcqKarglNK1jK5SKfAZ8fzn/Z6nnS6ODi1rXcLx6Y+P/5s9GwiZ3wXzj0ehChANV0UvIdyP0vc\nRg8/5VAzslibMg8oOFBNFwXP1wHn/zemrOTp31GKmCeSMydNPlm5qYRaiu1QcKCaLgqevrZkzNdh\nPGoV4bMhNmPkjUS8uhoyvQ7UZ1D2Sp7uT+9l7/UmZDtqupIfP36EdwullxydCjVaXhRr7T2E8INz\nbq2t1MPZm9I97Glr8h1V5/eaB6BsRYgCVDN8NaGExLAHpcRaO977KntnFB+FggPV4KXjRqRJcJ5n\nPMstOL1Zy2pPEhhVfBQO3hjnXJebJjG0k9SWMxCiANVAwcFb5MkitdJzBlBwoBooOGhKHC6dOfGV\nAgUHqoGCgymMGoGBg4OmPAtNRi+WQ4gCVFO1mtBa+8sY87Nfc7rzPYTwjXMibF2KU1urHByA1UCI\nAlQDBweqgYMD1cDBgWrg4EA1cHCgGjg4UA0cHKgGDg5U8x+ushcus5apKgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 216x216 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LbAL_Ye5Y8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = np.zeros((9, height, width, n_channel), dtype='float32')\n",
        "samples_labels = (np.ones((9)) * 5).astype('int32')\n",
        "for i in range(height):\n",
        "    for j in range(width):\n",
        "        logits = pixelcnn([samples, samples_labels])\n",
        "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])  # shape [N,H,W,DC] -> [N,H,W,D,C]\n",
        "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])  # shape [N,H,W,D,C] -> [N,H,W,C,D]\n",
        "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
        "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zLgN2mg5dX0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "7965ffc4-0f39-42d3-a9ca-171fc7d1a9e1"
      },
      "source": [
        "\n",
        "fig = plt.figure(figsize=(3, 3))\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3, 3, i + 1)\n",
        "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
        "    plt.xticks(np.array([]))\n",
        "    plt.yticks(np.array([]))\n",
        "plt.show()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAAC4CAYAAABQMybHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 216x216 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}