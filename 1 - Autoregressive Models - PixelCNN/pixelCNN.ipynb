{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pixelCNN.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bU25WyouYYE3"
   },
   "source": [
    "# Autoregressive Models - PixelCNN\n",
    "\n",
    "The autoregressive models are generative models that provides an explicit parametric specification of a likelihood function of a variable by specifying it with its parameters. However, to model data that have several dimensions/features, autoregressive models need to define some constraints. First, the observation space X need to have a determining ordering for its features. Second, to tractably model the joint distribution of the features in a data observation ($p(x)$), the autoregressive approach cast $p(x)$ as a product of conditional distributions. Autoregressive models define the joint distribution using conditionals over each feature given the values of the previous features. This way, autoregressive models use the chain rule to decompose the likelihood of the data sample x into a product of 1-dimensional distributions. The factorization turns the joint modelling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels.\n",
    "\n",
    "<center><img src=\"https://github.com/Mind-the-Pineapple/Autoregressive-models/blob/master/1%20-%20Autoregressive%20Models%20-%20PixelCNN/figures/Figure3.png?raw=1\" width=\"250\"  align=\"middle\"></center>\n",
    "\n",
    "\n",
    "## PixelCNN\n",
    "PixelCNN was introduced by DeepMind in 2016 (Oord et al, 2016) and it started one of the most promising families of autoregressive generative models. PixelCNN is a deep neural network that captures the distribution of dependencies between pixels in the parameters. It sequentially generates one pixel at a time in an image along the two spatial dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10r26ZgidIb5"
   },
   "source": [
    "In this implementation, we used the new Tensorflow 2.0 framework. In the first step we install and import the code dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "f25me9_Wpg3A",
    "outputId": "d61e5415-faa3-4ff8-8743-ef2d69c0f323",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "%tensorflow_version 2.x"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Syt9hLISPC1p",
    "colab": {}
   },
   "source": [
    "import random as rn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.utils import Progbar"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpYcifvjdbaA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we define the random seeds to have reproducible results and we load the MNIST dataset to train the PixelCNN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "JB9UVlLzPWI_",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Defining random seeds\n",
    "random_seed = 42\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "rn.seed(random_seed)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "iLud0DUePdyI",
    "colab": {}
   },
   "source": [
    "# Loading data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "n_channel = 1\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], height, width, n_channel)\n",
    "x_test = x_test.reshape(x_test.shape[0], height, width, n_channel)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnT0tbiyduyN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this example, to make the probability distribution of a single pixel easier to be defined, we decide to quantitise the number of possible values that a pixel could have. Originally, in the MNIST dataset the pixels are represented by a uint8 variable, beeing able assume values between [0, 255]. In this example, we restrict the image to have only 2 different values ([0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "gwfeosbzPOeo",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "def quantise(images, q_levels):\n",
    "    \"\"\"Quantise image into q levels\"\"\"\n",
    "    return (np.digitize(images, np.arange(q_levels) / q_levels) - 1).astype('float32')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "8h806UxIPkKr",
    "colab": {}
   },
   "source": [
    "# Quantise the input data in q levels\n",
    "q_levels = 2\n",
    "x_train_quantised = quantise(x_train, q_levels)\n",
    "x_test_quantised = quantise(x_test, q_levels)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3twbZSiGfBQP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Using the tensorflow.Data API, we defined the input data streams for our model during the training and the evaluation. In these dataset, we define the inputs as the images with 2 levels normalized to be between [0, 1] and the target values are the categoricals pixels values between [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "eQrNmco9Pxzo",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Creating input stream using tf.data API\n",
    "batch_size = 128\n",
    "train_buf = 60000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_quantised / (q_levels - 1),\n",
    "                                                    x_train_quantised.astype('int32')))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_buf)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_quantised / (q_levels - 1),\n",
    "                                                   x_test_quantised.astype('int32')))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGUFT_0eZGIY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Masked Convolutions\n",
    "\n",
    "When using the convolution operations, PixelCNN can parallelly learn the distribution of all pixels in the image. However, the receptive field of a standard convolutional violates the sequential prediction of autoregressive models. When processing the data of a central pixel, the convolutional filter considers all the pixels around it to calculate the output feature map, not only the previous pixels. \n",
    "\n",
    "To solve this problem, **masks** are adopted to block information flow from the future pixels.\n",
    "\n",
    "Masking can be done by zeroing out all the pixels that should not be considered. In our implementation, a mask with the same size to the convolutional filter with values 1 and 0 was created. This mask was multiplied with the weight tensor before doing the convolution operation.\n",
    "\n",
    "In the pixelCNN, there are two types of masks: type A and type B.\n",
    "\n",
    "*\t**Mask A**: this mask is applied only to the first convolutional layer. It restricts access to the pixel of interest by zeroing the center pixel in mask. This way, we guarantee model will not access the pixel that it is about to predict. \n",
    "\n",
    "*\t**Mask B**: This mask is applied to all the subsequent convolutional layers and relaxes the restrictions of mask A by allowing the connection from a pixel to itself. \n",
    "\n",
    "Below, we have the masks defined inside a keras layer named MaskedConv2D. In this layer, besides the parameters weigths and bias, we have the constant mask defining the pixels to be zeroed and the type of mask (A or B) (Lines 60 to 66).\n",
    "\n",
    "Finally, the mask is applied before the convolution (Line 69).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "qxhdp6wqPESj",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class MaskedConv2D(keras.layers.Layer):\n",
    "    \"\"\"Convolutional layers with masks.\n",
    "\n",
    "    Convolutional layers with simple implementation of masks type A and B for\n",
    "    autoregressive models.\n",
    "\n",
    "    Arguments:\n",
    "    mask_type: one of `\"A\"` or `\"B\".`\n",
    "    filters: Integer, the dimensionality of the output space\n",
    "        (i.e. the number of output filters in the convolution).\n",
    "    kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
    "        height and width of the 2D convolution window.\n",
    "        Can be a single integer to specify the same value for\n",
    "        all spatial dimensions.\n",
    "    strides: An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the height and width.\n",
    "        Can be a single integer to specify the same value for\n",
    "        all spatial dimensions.\n",
    "        Specifying any stride value != 1 is incompatible with specifying\n",
    "        any `dilation_rate` value != 1.\n",
    "    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
    "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
    "    bias_initializer: Initializer for the bias vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mask_type,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros'):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding.upper()\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight('kernel',\n",
    "                                      shape=(self.kernel_size,\n",
    "                                             self.kernel_size,\n",
    "                                             int(input_shape[-1]),\n",
    "                                             self.filters),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight('bias',\n",
    "                                    shape=(self.filters,),\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    trainable=True)\n",
    "\n",
    "        center = self.kernel_size // 2\n",
    "\n",
    "        mask = np.ones(self.kernel.shape, dtype=np.float32)\n",
    "        mask[center, center + (self.mask_type == 'B'):, :, :] = 0.\n",
    "        mask[center + 1:, :, :, :] = 0.\n",
    "\n",
    "        self.mask = tf.constant(mask, dtype=tf.float32, name='mask')\n",
    "\n",
    "    def call(self, input):\n",
    "        masked_kernel = tf.math.multiply(self.mask, self.kernel)\n",
    "        x = nn.conv2d(input,\n",
    "                      masked_kernel,\n",
    "                      strides=[1, self.strides, self.strides, 1],\n",
    "                      padding=self.padding)\n",
    "        x = nn.bias_add(x, self.bias)\n",
    "        return x"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoCfR38cZKDC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Residual blocks\n",
    "\n",
    "Another element that compose the PixelCNN architecture is the residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "vzvk2UL5PI6k",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class ResidualBlock(keras.Model):\n",
    "    \"\"\"Residual blocks that compose pixelCNN\n",
    "\n",
    "    Blocks of layers with 3 convolutional layers and one residual connection.\n",
    "    Based on Figure 5 from [1] where h indicates number of filters.\n",
    "\n",
    "    Refs:\n",
    "    [1] - Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). Pixel recurrent\n",
    "    neural networks. arXiv preprint arXiv:1601.06759.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h):\n",
    "        super(ResidualBlock, self).__init__(name='')\n",
    "\n",
    "        self.conv2a = keras.layers.Conv2D(filters=h, kernel_size=1, strides=1)\n",
    "        self.conv2b = MaskedConv2D(mask_type='B', filters=h, kernel_size=3, strides=1)\n",
    "        self.conv2c = keras.layers.Conv2D(filters=2 * h, kernel_size=1, strides=1)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = nn.relu(input_tensor)\n",
    "        x = self.conv2a(x)\n",
    "\n",
    "        x = nn.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "\n",
    "        x = nn.relu(x)\n",
    "        x = self.conv2c(x)\n",
    "\n",
    "        x += input_tensor\n",
    "        return x"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5aGeua5ZNzz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PixelCNN architecture\n",
    "\n",
    "In Oord et al. 2016, the PixelCNN uses the following architecture.\n",
    "\n",
    "<center><img src=\"https://github.com/Mind-the-Pineapple/Autoregressive-models/blob/master/1%20-%20Autoregressive%20Models%20-%20PixelCNN/figures/Figure5_Architecture.png?raw=1\" width=\"700\"></center>\n",
    "\n",
    "The first layer is a masked convolution (type A) with 7x7 filters. Then, 15 residual blocks were used. Each block processes the data with a combination of 3x3 layers convolutional layers with mask type B and 1x1 standard convolutional layers. Between each convolutional layer there is also a non-linearity ReLU. After these layers, a residual connection is added. Finally, the network has a sequence of RELU-CONV-RELU-CONV using standard convolutional layers, where both have 1x1 filters. Then, the output layer is a softmax layer which predicts the value among all possible values of a pixel. So, this softmax layer had the image dimension (because we want an output value for each pixel) times the number of possible values (for example, 256 pixels values)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "F7jBV3YUP7WB",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Create PixelCNN model\n",
    "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
    "x = MaskedConv2D(mask_type='A', filters=128, kernel_size=7, strides=1)(inputs)\n",
    "\n",
    "for i in range(15):\n",
    "    x = ResidualBlock(h=64)(x)\n",
    "\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=q_levels, kernel_size=1, strides=1)(x)\n",
    "\n",
    "pixelcnn = keras.Model(inputs=inputs, outputs=x)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ5d_FaDh9gY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this implementation we use a simple Adam optimizer with learning rate decay to train the neural network. The loss function is defined by the cross-entropy (that in this case is equivalent to minimizing the negative log-likelihood of the training data)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ChbH5SB4P-k1",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Prepare optimizer and loss function\n",
    "lr_decay = 0.999995\n",
    "learning_rate = 1e-3\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "compute_loss = keras.losses.CategoricalCrossentropy(from_logits=True)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQRb1AD8iV4S",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The training step is defined by the forward propagation through the model. Then, the gradients are calculated, clipped to be between [-1, 1], and applied to upgrade the PixelCNN parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HUBXLETLQA81",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        logits = pixelcnn(batch_x, training=True)\n",
    "\n",
    "        loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    gradients = ae_tape.gradient(loss, pixelcnn.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "    optimizer.apply_gradients(zip(gradients, pixelcnn.trainable_variables))\n",
    "\n",
    "    return loss"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4d0Qwy-jA10",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this implementation, we defined the training loop with 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "hpu7iZrSQErQ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "outputId": "4f380902-8160-4b08-c055-ac12435305e6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = int(np.ceil(x_train_quantised.shape[0] / batch_size))\n",
    "for epoch in range(n_epochs):\n",
    "    progbar = Progbar(n_iter)\n",
    "    print('Epoch {:}/{:}'.format(epoch + 1, n_epochs))\n",
    "\n",
    "    for i_iter, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        optimizer.lr = optimizer.lr * lr_decay\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "\n",
    "        progbar.add(1, values=[('loss', loss)])"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "469/469 [==============================] - 70s 150ms/step - loss: 0.1013\n",
      "Epoch 2/100\n",
      "469/469 [==============================] - 64s 137ms/step - loss: 0.0855\n",
      "Epoch 3/100\n",
      "469/469 [==============================] - 64s 137ms/step - loss: 0.0830\n",
      "Epoch 4/100\n",
      "469/469 [==============================] - 64s 136ms/step - loss: 0.0812\n",
      "Epoch 5/100\n",
      "396/469 [========================>.....] - ETA: 9s - loss: 0.0799 "
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-98c1fb2b10eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, n, values)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seen_so_far\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    488\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    913\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1199\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   6112\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6113\u001b[0;31m         x, y)\n\u001b[0m\u001b[1;32m   6114\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6115\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUVLFcETjH0h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To evaluate the performance of the model, we measured its negative log-likelihood (NLL) in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "wwOpkugDVQuo",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Test set performance\n",
    "test_loss = []\n",
    "for batch_x, batch_y in test_dataset:\n",
    "    logits = pixelcnn(batch_x, training=False)\n",
    "\n",
    "    # Calculate cross-entropy (= negative log-likelihood)\n",
    "    loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    test_loss.append(loss)\n",
    "print('nll : {:} nats'.format(np.array(test_loss).mean()))\n",
    "print('bits/dim : {:}'.format(np.array(test_loss).mean() / np.log(2)))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riO4FtjKjRUR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, we sampled some images from the trained model. First, we sampled from scratch, then we completed images partially occluded."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "UXQ_GbDPVdiU",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Generating new images\n",
    "samples = np.zeros((100, height, width, n_channel), dtype='float32')\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "uJahzGf3XjjM",
    "colab": {}
   },
   "source": [
    "# Filling occluded images\n",
    "occlude_start_row = 14\n",
    "num_generated_images = 10\n",
    "samples = np.copy(x_test_quantised[0:num_generated_images, :, :, :])\n",
    "samples = samples / (q_levels - 1)\n",
    "samples[:, occlude_start_row:, :, :] = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "\n",
    "for i in range(occlude_start_row, height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdHSvSh4v8r-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That's looks great, so now let's check with 256 levels."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ak4cTvpMIE9z",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Quantise the input data in q levels\n",
    "q_levels = 256\n",
    "x_train_quantised = quantise(x_train, q_levels)\n",
    "x_test_quantised = quantise(x_test, q_levels)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Creating input stream using tf.data API\n",
    "batch_size = 128\n",
    "train_buf = 60000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_quantised / (q_levels - 1),\n",
    "                                                    x_train_quantised.astype('int32')))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_buf)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_quantised / (q_levels - 1),\n",
    "                                                   x_test_quantised.astype('int32')))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Create PixelCNN model\n",
    "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
    "x = MaskedConv2D(mask_type='A', filters=128, kernel_size=7, strides=1)(inputs)\n",
    "\n",
    "for i in range(15):\n",
    "    x = ResidualBlock(h=64)(x)\n",
    "\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=q_levels, kernel_size=1, strides=1)(x)\n",
    "\n",
    "pixelcnn = keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Prepare optimizer and loss function\n",
    "lr_decay = 0.999995\n",
    "learning_rate = 1e-3\n",
    "optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "compute_loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        logits = pixelcnn(batch_x, training=True)\n",
    "\n",
    "        loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    gradients = ae_tape.gradient(loss, pixelcnn.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "    optimizer.apply_gradients(zip(gradients, pixelcnn.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = int(np.ceil(x_train_quantised.shape[0] / batch_size))\n",
    "for epoch in range(n_epochs):\n",
    "    progbar = Progbar(n_iter)\n",
    "    print('Epoch {:}/{:}'.format(epoch + 1, n_epochs))\n",
    "\n",
    "    for i_iter, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        optimizer.lr = optimizer.lr * lr_decay\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "\n",
    "        progbar.add(1, values=[('loss', loss)])\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Test set performance\n",
    "test_loss = []\n",
    "for batch_x, batch_y in test_dataset:\n",
    "    logits = pixelcnn(batch_x, training=False)\n",
    "\n",
    "    # Calculate cross-entropy (= negative log-likelihood)\n",
    "    loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    test_loss.append(loss)\n",
    "print('nll : {:} nats'.format(np.array(test_loss).mean()))\n",
    "print('bits/dim : {:}'.format(np.array(test_loss).mean() / np.log(2)))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9cwrWGLlhyw6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Generating new images\n",
    "samples = np.zeros((100, height, width, n_channel), dtype='float32')\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h6o4QILaiBil",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Filling occluded images\n",
    "occlude_start_row = 14\n",
    "num_generated_images = 10\n",
    "samples = np.copy(x_test_quantised[0:num_generated_images, :, :, :])\n",
    "samples = samples / (q_levels - 1)\n",
    "samples[:, occlude_start_row:, :, :] = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "\n",
    "for i in range(occlude_start_row, height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:, 0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i + 1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RusOD7aSFpAF",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because of the higher number of dimensions, the challenge of predicting the conditional probability becomes harder, but even so, the results look very interesting! This is the very first PixelCNN model and a few other implementations have been released that use novel and state-of-the-art techniques to obtain great results. We are going to delve into them in future notebooks/blogposts.\n"
   ]
  }
 ]
}