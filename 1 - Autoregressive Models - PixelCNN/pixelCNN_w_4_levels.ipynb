{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Warvito/Autoregressive-models/blob/master/1%20-%20Autoregressive%20Models%20-%20PixelCNN/pixelCNN_w_4_levels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bU25WyouYYE3"
   },
   "source": [
    "# Autoregressive Models - PixelCNN\n",
    "\n",
    "The autoregressive models are generative models that provides an explicit parametric specification of a likelihood function of a variable by specifying it with its parameters. However, to model data that have several dimensions/features, autoregressive models need to define some constraints. First, the observation space X need to have a determining ordering for its features. That is why autoregressive models are naturally used for time series that have a sequence of time steps. However, it can also be employed for images by defining, for example, that the pixels from left come before from the ones in the right, and the ones in the top before the ones in the bottom. Second, to tractably model the joint distribution of the features in a data observation (p(x)), the autoregressive approach cast p(x) as a product of conditional distributions. Autoregressive models define the joint distribution using conditionals over each feature given the values of the previous features. For example, the probability of a pixel from an image to have a specific value is conditioned by the values of all previous pixels; and the probability of an image (the joint distribution) is the combination of the probability of all its pixels. This way, autoregressive models use the chain rule to decompose the likelihood of the data sample x into a product of 1-dimensional distributions (Eq. 1). The factorization turns the joint modelling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels.\n",
    "\n",
    "***IMAGE***\n",
    "\n",
    "This is what mainly define an autoregressive model.\n",
    "\n",
    "Now, the big challenge is how to represent these conditional this p(xi) that usually has a really complex distribution. How could we define it in an expressive model that are also tractable and scalable? One solution is using deep neural networks. \n",
    "\n",
    "\n",
    "## PixelCNN\n",
    "PixelCNN was introduced by DeepMind in 2016 (Oord et al, 2016) and it started one of the most promising families of autoregressive generative models. PixelCNN is a deep neural network that captures the distribution of dependencies between pixels in the parameters. It sequentially generates one pixel at a time in an image along the two spatial dimensions. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "10r26ZgidIb5"
   },
   "source": [
    "In this implementation, we used the new Tensorflow 2.0 framework. In the fisrt step we install and import the code dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f25me9_Wpg3A"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Syt9hLISPC1p"
   },
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Progbar\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpYcifvjdbaA"
   },
   "source": [
    "Then, we define the random seeds to have reproducible results and we load the MNIST dataset to train the PixelCNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JB9UVlLzPWI_"
   },
   "outputs": [],
   "source": [
    "# Defining random seeds\n",
    "random_seed = 42\n",
    "tf.random.set_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "rn.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLud0DUePdyI"
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "n_channel = 1\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], height, width, n_channel)\n",
    "x_test = x_test.reshape(x_test.shape[0], height, width, n_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnT0tbiyduyN"
   },
   "source": [
    "In this example, to make the probability distribution of a single pixel easier to be defined, we decide to quantisise the number of possible values that a pixel could have. Originally, in the MNIST dataset the pixel are represented bya  a uint8 variable, beeing able assume values between [0, 255]. In this example, we restring the image to have only 4 different values ([0, 3])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gwfeosbzPOeo"
   },
   "outputs": [],
   "source": [
    "def quantise(images, q_levels):\n",
    "    \"\"\"Quantise image into q levels\"\"\"\n",
    "    return (np.digitize(images, np.arange(q_levels) / q_levels) - 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8h806UxIPkKr"
   },
   "outputs": [],
   "source": [
    "# Quantise the input data in q levels\n",
    "q_levels = 4\n",
    "x_train_quantised = quantise(x_train, q_levels)\n",
    "x_test_quantised = quantise(x_test, q_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3twbZSiGfBQP"
   },
   "source": [
    "Using the tensorflow.Data API, we defined the input data streams for our model during the training and the evaluation. In these dataset, we define the inputs as the images with the 4 levels normaized to be between [0, 1] and the target values are the categoricals pixels values between [0, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQrNmco9Pxzo"
   },
   "outputs": [],
   "source": [
    "# Creating input stream using tf.data API\n",
    "batch_size = 128\n",
    "train_buf = 60000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_quantised / (q_levels - 1),\n",
    "                                                    x_train_quantised.astype('int32')))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_buf)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_quantised / (q_levels - 1),\n",
    "                                                   x_test_quantised.astype('int32')))\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGUFT_0eZGIY"
   },
   "source": [
    "## Masked Convolutions\n",
    "\n",
    "When using the convolution operations, PixelCNN can parallelly learn the distribution of all pixels in the image. However, the receptive field of a standard convolutional violates the sequential prediction of autoregressive models. When processing the data of a central pixel, the convolutional filter considers all the pixels around it to calculate the output feature map, not only the previous pixels. \n",
    "\n",
    "To solve this problem, masks are adopted block information flow from the future pixels.\n",
    "\n",
    "Masking can be done by zeroing out all the pixels that should not be considered. In our implementation, a mask with the same size to the convolutional filter with values 1 and 0 was created. This mask was multiplied with the weight tensor before doing the convolution operation.\n",
    "\n",
    "In the pixelCNN, there are two types of masks: type A and type B.\n",
    "\n",
    "*\tMask A: this mask is applied only to the first convolutional layer. It restricts to access to the pixel of interest by zeroing the center pixel in mask. This way, we guarantee model will not access the pixel that it is about to predict. \n",
    "\n",
    "*\tMask B: This mask is applied to all the subsequent convolutional layers and relaxes the restrictions of mask A by allowing the connection from a pixel to itself. \n",
    "\n",
    "Below, we have the masks defined inside a keras layer named MasekdConv2D. In this layer, besides the parameters weigths and bias, we have the constant mask defininng the pixels to be zeroed and the type of mask (A or B) (Lines 41 to 47).\n",
    "\n",
    "Finally, the mask is applied before the convolution (Line 50).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qxhdp6wqPESj"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2D(tf.keras.layers.Layer):\n",
    "    \"\"\"Convolutional layers with masks for autoregressive models\n",
    "\n",
    "    Convolutional layers with simple implementation to have masks type A and B.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mask_type,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=1,\n",
    "                 padding='same',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros'):\n",
    "        super(MaskedConv2D, self).__init__()\n",
    "\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding.upper()\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=(self.kernel_size,\n",
    "                                             self.kernel_size,\n",
    "                                             int(input_shape[-1]),\n",
    "                                             self.filters),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      trainable=True)\n",
    "\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                    shape=(self.filters,),\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    trainable=True)\n",
    "\n",
    "        center = self.kernel_size // 2\n",
    "\n",
    "        mask = np.ones(self.kernel.shape, dtype=np.float32)\n",
    "        mask[center, center + (self.mask_type == 'B'):, :, :] = 0.\n",
    "        mask[center + 1:, :, :, :] = 0.\n",
    "\n",
    "        self.mask = tf.constant(mask, dtype=tf.float32, name='mask')\n",
    "\n",
    "    def call(self, input):\n",
    "        masked_kernel = tf.math.multiply(self.mask, self.kernel)\n",
    "        x = tf.nn.conv2d(input, masked_kernel, strides=[1, self.strides, self.strides, 1], padding=self.padding)\n",
    "        x = tf.nn.bias_add(x, self.bias)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LoCfR38cZKDC"
   },
   "source": [
    "## Residual blocks\n",
    "\n",
    "Another element that compose the PixelCNN architecture is the residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vzvk2UL5PI6k"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.Model):\n",
    "    \"\"\"Residual blocks that compose pixelCNN\n",
    "\n",
    "    Blocks of layers with 3 convolutional layers and one residual connection.\n",
    "    Based on Figure 5 from [1] where h indicates number of filters.\n",
    "\n",
    "    Refs:\n",
    "    [1] - Oord, A. V. D., Kalchbrenner, N., & Kavukcuoglu, K. (2016). Pixel\n",
    "     recurrent neural networks. arXiv preprint arXiv:1601.06759.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h):\n",
    "        super(ResidualBlock, self).__init__(name='')\n",
    "\n",
    "        self.conv2a = keras.layers.Conv2D(filters=h, kernel_size=1, strides=1)\n",
    "        self.conv2b = MaskedConv2D(mask_type='B', filters=h, kernel_size=3, strides=1)\n",
    "        self.conv2c = keras.layers.Conv2D(filters=2 * h, kernel_size=1, strides=1)\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        x = tf.nn.relu(input_tensor)\n",
    "        x = self.conv2a(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv2c(x)\n",
    "\n",
    "        x += input_tensor\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5aGeua5ZNzz"
   },
   "source": [
    "## PixelCNN architecture\n",
    "\n",
    "In Oord et al. 2016, the PixelCNN uses the following architecture (Figure X):\n",
    "\n",
    "<img src=\"figures/Figure5_Architecture.png\" width=\"800\">\n",
    "\n",
    "the first layer is a masked convolution (type A) with 7x7 filters. Then, 15 residuals blocks were used. Each block process the data with a combination of 3x3 layers convolutional layers with mask type B and 1x1 standard convolutional layers. Between each convolutional layer there is also a non-linearity ReLU. After these layers, a residual connection is added. Finally, the network has a sequence of RELU-CONV-RELU-CONV using standard convolutional layers, where both have 1x1 filters. Then, the output layer is a softmax layer which predicts the value among all possible values of a pixel. So, this softmax layer had the image dimension (because we want an output value for each pixel) times the number of possible values (for example, 256 pixels values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F7jBV3YUP7WB"
   },
   "outputs": [],
   "source": [
    "# Create PixelCNN model\n",
    "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
    "x = MaskedConv2D(mask_type='A', filters=128, kernel_size=7, strides=1)(inputs)\n",
    "\n",
    "for i in range(15):\n",
    "    x = ResidualBlock(h=64)(x)\n",
    "\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
    "x = keras.layers.Conv2D(filters=n_channel * q_levels, kernel_size=1, strides=1)(x)  # shape [N,H,W,DC]\n",
    "\n",
    "pixelcnn = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQ5d_FaDh9gY"
   },
   "source": [
    "In this implementation we use a simple Adam optimizer with learning rate decay to train the neural network. The loss function is defined by the crossentropy (that in this case is equivalent to minimizing the negative log-likelihood of the training data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChbH5SB4P-k1"
   },
   "outputs": [],
   "source": [
    "# Prepare optimizer and loss function\n",
    "lr_decay = 0.9999\n",
    "learning_rate = 1e-2\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "compute_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQRb1AD8iV4S"
   },
   "source": [
    "The training step is defined by the forward propagation throgh the model, and then the outputs are reshaped to work with several inputs channels (like RGB channels). Finally, the gradients are calculated, clipped to be between [-1, 1], and applied to upgrade the PixelCNN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUBXLETLQA81"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        logits = pixelcnn(batch_x, training=True)\n",
    "\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])  # shape [N,H,W,DC] -> [N,H,W,D,C]\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])  # shape [N,H,W,D,C] -> [N,H,W,C,D]\n",
    "\n",
    "        loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    gradients = ae_tape.gradient(loss, pixelcnn.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "    optimizer.apply_gradients(zip(gradients, pixelcnn.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4d0Qwy-jA10"
   },
   "source": [
    "In this implementation, we defined the trainig loop with 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hpu7iZrSQErQ",
    "outputId": "7faa09c6-0cac-47ce-abf8-7a371ba422a5"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = int(np.ceil(x_train_quantised.shape[0] / batch_size))\n",
    "for epoch in range(n_epochs):\n",
    "    progbar = Progbar(n_iter)\n",
    "    print('Epoch {:}/{:}'.format(epoch+1, n_epochs))\n",
    "        \n",
    "    for i_iter, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        start = time.time()\n",
    "        optimizer.lr = optimizer.lr * lr_decay\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "\n",
    "        progbar.add(1, values=[(\"loss\", loss)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUVLFcETjH0h"
   },
   "source": [
    "To evaluate the performance of the model, we measured the NLL of the model in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wwOpkugDVQuo",
    "outputId": "0fc0fb98-3a1e-4115-c275-0c1698bf33c6"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "test_loss = []\n",
    "for batch_x, batch_y in test_dataset:\n",
    "    logits = pixelcnn(batch_x, training=False)\n",
    "    logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "    logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "\n",
    "    # Calculate cross-entropy (= negative log-likelihood)\n",
    "    loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    test_loss.append(loss)\n",
    "print('nll : {:} nats'.format(np.array(test_loss).mean()))\n",
    "print('bits/dim : {:}'.format(np.array(test_loss).mean() / (height * width)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "riO4FtjKjRUR"
   },
   "source": [
    "Finally, we sampled some images from the trained model. First, we sampled from the scratch, then we completed imaged partially occluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "UXQ_GbDPVdiU",
    "outputId": "6422a600-c27c-4f37-dc90-3fe266368bb4"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Generating new images\n",
    "samples = np.zeros((100, height, width, n_channel), dtype='float32')\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:,0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "colab_type": "code",
    "id": "uJahzGf3XjjM",
    "outputId": "7e715032-b0a0-4e6b-81bb-4ce696905f03"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Filling occluded images\n",
    "occlude_start_row = 14\n",
    "num_generated_images = 10\n",
    "samples = np.copy(x_test_quantised[0:num_generated_images, :, :, :])\n",
    "samples = samples / (q_levels - 1)\n",
    "samples[:, occlude_start_row:, :, :] = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "\n",
    "for i in range(occlude_start_row, height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:,0]\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdHSvSh4v8r-"
   },
   "source": [
    "That's looks great, so now let's check with 256 levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJNxD5nOv6-a",
    "outputId": "ddeb245a-a7a2-4413-802b-1e77b7570950"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Quantise the input data in q levels\n",
    "q_levels = 256\n",
    "x_train_quantised = quantise(x_train, q_levels)\n",
    "x_test_quantised = quantise(x_test, q_levels)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Creating input stream using tf.data API\n",
    "batch_size = 128\n",
    "train_buf = 60000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_quantised / (q_levels - 1),\n",
    "                                                    x_train_quantised.astype('int32')))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_buf)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_quantised / (q_levels - 1),\n",
    "                                                   x_test_quantised.astype('int32')))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Create PixelCNN model\n",
    "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
    "x = MaskedConv2D(mask_type='A', filters=128, kernel_size=7, strides=1)(inputs)\n",
    "\n",
    "for i in range(15):\n",
    "    x = ResidualBlock(h=64)(x)\n",
    "\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=128, kernel_size=1, strides=1)(x)\n",
    "x = keras.layers.Activation(activation='relu')(x)\n",
    "x = keras.layers.Conv2D(filters=n_channel * q_levels, kernel_size=1, strides=1)(x)  # shape [N,H,W,DC]\n",
    "\n",
    "pixelcnn = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Prepare optimizer and loss function\n",
    "lr_decay = 0.9999\n",
    "learning_rate = 5e-3\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "compute_loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "@tf.function\n",
    "def train_step(batch_x, batch_y):\n",
    "    with tf.GradientTape() as ae_tape:\n",
    "        logits = pixelcnn(batch_x, training=True)\n",
    "\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "\n",
    "        loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    gradients = ae_tape.gradient(loss, pixelcnn.trainable_variables)\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "    optimizer.apply_gradients(zip(gradients, pixelcnn.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = int(np.ceil(x_train_quantised.shape[0] / batch_size))\n",
    "for epoch in range(n_epochs):\n",
    "    start_epoch = time.time()\n",
    "    for i_iter, (batch_x, batch_y) in enumerate(train_dataset):\n",
    "        start = time.time()\n",
    "        optimizer.lr = optimizer.lr * lr_decay\n",
    "        loss = train_step(batch_x, batch_y)\n",
    "        iter_time = time.time() - start\n",
    "        if i_iter % 100 == 0:\n",
    "            print('EPOCH {:3d}: ITER {:4d}/{:4d} TIME: {:.2f} LOSS: {:.4f}'.format(epoch,\n",
    "                                                                                   i_iter, n_iter,\n",
    "                                                                                   iter_time,\n",
    "                                                                                   loss))\n",
    "    epoch_time = time.time() - start_epoch\n",
    "    print('EPOCH {:3d}: TIME: {:.2f} ETA: {:.2f}'.format(epoch,\n",
    "                                                         epoch_time,\n",
    "                                                         epoch_time * (n_epochs - epoch)))\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Test\n",
    "test_loss = []\n",
    "for batch_x, batch_y in test_dataset:\n",
    "    logits = pixelcnn(batch_x, training=False)\n",
    "    logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "    logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "\n",
    "    # Calculate cross-entropy (= negative log-likelihood)\n",
    "    loss = compute_loss(tf.one_hot(batch_y, q_levels), logits)\n",
    "\n",
    "    test_loss.append(loss)\n",
    "print('nll : {:} nats'.format(np.array(test_loss).mean()))\n",
    "print('bits/dim : {:}'.format(np.array(test_loss).mean() / (height * width)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgPsdpIxwe5X",
    "outputId": "55d5021b-9ca0-4542-a952-ef72afe83a31"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Generating new images\n",
    "samples = np.zeros((100, height, width, n_channel), dtype='float32')\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:,0]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(10, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nD9V7ANUwgIg",
    "outputId": "da09bc1c-0117-4c09-cabb-8b327905cf2d"
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------\n",
    "# Filling occluded images\n",
    "occlude_start_row = 14\n",
    "num_generated_images = 10\n",
    "samples = np.copy(x_test_quantised[0:num_generated_images, :, :, :])\n",
    "samples = samples / (q_levels - 1)\n",
    "samples[:, occlude_start_row:, :, :] = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "\n",
    "for i in range(occlude_start_row, height):\n",
    "    for j in range(width):\n",
    "        logits = pixelcnn(samples)\n",
    "        logits = tf.reshape(logits, [-1, height, width, q_levels, n_channel])\n",
    "        logits = tf.transpose(logits, perm=[0, 1, 2, 4, 3])\n",
    "        next_sample = tf.random.categorical(logits[:, i, j, 0, :], 1)\n",
    "        samples[:, i, j, 0] = (next_sample.numpy() / (q_levels - 1))[:,0]\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1, 10, i+1)\n",
    "    ax.matshow(samples[i, :, :, 0], cmap=matplotlib.cm.binary)\n",
    "    plt.xticks(np.array([]))\n",
    "    plt.yticks(np.array([]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "pixelCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
