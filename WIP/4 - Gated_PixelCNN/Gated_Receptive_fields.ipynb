{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gated Receptive fields.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqTKIYLooHsq",
        "colab_type": "text"
      },
      "source": [
        "# Comparing receptive fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf5wwqP3ozaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random as rn\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow import nn\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.utils import Progbar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEkll1yno2Vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining random seeds\n",
        "random_seed = 42\n",
        "tf.random.set_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "rn.seed(random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_JlzWco7ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedConv2D(keras.layers.Layer):\n",
        "    \"\"\"Convolutional layers with masks for Gated PixelCNN.\n",
        "\n",
        "    Masked convolutional layers used to implement Vertical and Horizontal\n",
        "    stacks of the Gated PixelCNN.\n",
        "\n",
        "    Note: This implementation is different from the normal PixelCNN.\n",
        "\n",
        "    Arguments:\n",
        "    mask_type: one of `\"V\"`, `\"A\"` or `\"B\".`\n",
        "    filters: Integer, the dimensionality of the output space\n",
        "        (i.e. the number of output filters in the convolution).\n",
        "    kernel_size: An integer or tuple/list of 2 integers, specifying the\n",
        "        height and width of the 2D convolution window.\n",
        "        Can be a single integer to specify the same value for\n",
        "        all spatial dimensions.\n",
        "    strides: An integer or tuple/list of 2 integers,\n",
        "        specifying the strides of the convolution along the height and width.\n",
        "        Can be a single integer to specify the same value for\n",
        "        all spatial dimensions.\n",
        "        Specifying any stride value != 1 is incompatible with specifying\n",
        "        any `dilation_rate` value != 1.\n",
        "    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n",
        "    kernel_initializer: Initializer for the `kernel` weights matrix.\n",
        "    bias_initializer: Initializer for the bias vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 mask_type,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 strides=1,\n",
        "                 padding='same',\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros'):\n",
        "        super(MaskedConv2D, self).__init__()\n",
        "\n",
        "        assert mask_type in {'A', 'B', 'V'}\n",
        "        self.mask_type = mask_type\n",
        "\n",
        "        self.filters = filters\n",
        "\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.strides = strides\n",
        "        self.padding = padding.upper()\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        kernel_h, kernel_w = self.kernel_size\n",
        "\n",
        "        self.kernel = self.add_weight('kernel',\n",
        "                                      shape=(kernel_h,\n",
        "                                             kernel_w,\n",
        "                                             int(input_shape[-1]),\n",
        "                                             self.filters),\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      trainable=True)\n",
        "\n",
        "        self.bias = self.add_weight('bias',\n",
        "                                    shape=(self.filters,),\n",
        "                                    initializer=self.bias_initializer,\n",
        "                                    trainable=True)\n",
        "\n",
        "        mask = np.ones(self.kernel.shape, dtype=np.float32)\n",
        "\n",
        "        if kernel_h % 2 != 0: \n",
        "            center_h = kernel_h // 2\n",
        "        else:\n",
        "            center_h = (kernel_h - 1) // 2\n",
        "\n",
        "        if kernel_w % 2 != 0: \n",
        "            center_w = kernel_w // 2\n",
        "        else:\n",
        "            center_w = (kernel_w - 1) // 2\n",
        "\n",
        "        if self.mask_type == 'V':\n",
        "            mask[center_h + 1:, :, :, :] = 0.\n",
        "        else:\n",
        "            mask[:center_h, :, :] = 0.\n",
        "            mask[center_h, center_w + (self.mask_type == 'B'):, :, :] = 0.\n",
        "            mask[center_h + 1:, :, :] = 0.          \n",
        "\n",
        "        self.mask = tf.constant(mask, dtype=tf.float32, name='mask')\n",
        "\n",
        "    def call(self, input):\n",
        "        masked_kernel = tf.math.multiply(self.mask, self.kernel)\n",
        "        x = nn.conv2d(input,\n",
        "                      masked_kernel,\n",
        "                      strides=[1, self.strides, self.strides, 1],\n",
        "                      padding=self.padding)\n",
        "        x = nn.bias_add(x, self.bias)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5topys_HW7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GatedBlock(tf.keras.Model):\n",
        "    \"\"\" Gated block of the Gated PixelCNN.\"\"\"\n",
        "\n",
        "    def __init__(self, mask_type, filters, kernel_size):\n",
        "        super(GatedBlock, self).__init__(name='')\n",
        "\n",
        "        self.mask_type = mask_type\n",
        "        self.vertical_conv = MaskedConv2D(mask_type='V',\n",
        "                                          filters=2 * filters,\n",
        "                                          kernel_size=kernel_size)\n",
        "        \n",
        "        self.horizontal_conv = MaskedConv2D(mask_type=mask_type,\n",
        "                                            filters=2 * filters,\n",
        "                                            kernel_size=kernel_size)\n",
        "\n",
        "        self.padding = keras.layers.ZeroPadding2D(padding=((1,0),0))\n",
        "        self.cropping = keras.layers.Cropping2D(cropping=((0, 1), 0))\n",
        "\n",
        "        self.v_to_h_conv = keras.layers.Conv2D(filters=2 * filters, kernel_size=1)\n",
        "\n",
        "        self.horizontal_output = keras.layers.Conv2D(filters=filters, kernel_size=1)\n",
        "\n",
        "    def _gate(self, x):\n",
        "        tanh_preactivation, sigmoid_preactivation = tf.split(x, 2, axis=-1)\n",
        "        return tf.nn.tanh(tanh_preactivation) * tf.nn.sigmoid(sigmoid_preactivation)\n",
        "\n",
        "    def call(self, input_tensor):\n",
        "        v = input_tensor[0]\n",
        "        h = input_tensor[1]\n",
        "\n",
        "        vertical_preactivation = self.vertical_conv(v)  # NxN\n",
        "\n",
        "        # Shifting feature map down to ensure causality\n",
        "        v_to_h = self.padding(vertical_preactivation)\n",
        "        v_to_h = self.cropping(v_to_h)\n",
        "        v_to_h = self.v_to_h_conv(v_to_h)  # 1x1\n",
        "\n",
        "        horizontal_preactivation = self.horizontal_conv(h)  # 1xN\n",
        "        \n",
        "        v_out = self._gate(vertical_preactivation)\n",
        "\n",
        "        horizontal_preactivation = horizontal_preactivation + v_to_h\n",
        "        h_activated = self._gate(horizontal_preactivation)\n",
        "        h_activated = self.horizontal_output(h_activated)\n",
        "\n",
        "        if self.mask_type == 'A':\n",
        "            h_out = h_activated\n",
        "        elif self.mask_type == 'B':\n",
        "            h_out = h + h_activated\n",
        "\n",
        "        return v_out, h_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxCLMYc-FxdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_receptive_field(model, data):\n",
        "    out = model(data)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(data)\n",
        "        prediction = model(data)\n",
        "        loss = prediction[:,5,5,0]\n",
        "\n",
        "    gradients = tape.gradient(loss, data)\n",
        "\n",
        "    gradients = np.abs(gradients.numpy().squeeze())\n",
        "    gradients = (gradients > 1e-8).astype('float32')\n",
        "    gradients[5, 5] = 0.5\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(gradients)\n",
        "    plt.title(f'Receptive field from pixel layers')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qpDtNuvo9NL",
        "colab_type": "code",
        "outputId": "b8142536-f9db-4ce3-ed7e-03741da11a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "inputs = keras.layers.Input(shape=(height, width, n_channel))\n",
        "v, h = GatedBlock(mask_type='A', filters=1, kernel_size=3)([inputs, inputs])\n",
        "v, h = GatedBlock(mask_type='B', filters=1, kernel_size=3)([v, h])\n",
        "v, h = GatedBlock(mask_type='B', filters=1, kernel_size=3)([v, h])\n",
        "v, h = GatedBlock(mask_type='B', filters=1, kernel_size=3)([v, h])\n",
        "v, h = GatedBlock(mask_type='B', filters=1, kernel_size=3)([v, h])\n",
        "model = tf.keras.Model(inputs=inputs, outputs=h)\n",
        "\n",
        "data = tf.random.normal((1,10,10,1))\n",
        "\n",
        "plot_receptive_field(model, data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2APaCzDGeqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}